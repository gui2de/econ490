##INCLUSION AND EXCLUSION ERROR IN CONDITIONAL CASH TRANSFER


The main trade-off governments face when designing benefits that target a certain portion of the population is the one between including a large number of beneficiaries that should not participate in the program versus excluding a large number of participants that should have been included. In slightly more technical jargon, we call these inclusion and exclusion error, respectively. 

This program is meant to test the amount of exclusion and inclusion error we can expect from using a Proxy Means Test (PMT) to estimate family income in a population. In short, PMT is a method that assigns weights to different characteristics of a household based on a linear regression model on a large sample to estimate household income. This sort of test is widely used to assess whether a household applying for government benefits meets a given income threshold, and it is broader than a more straightforward	approach based on asking "how much income did your household generate last month" and going from there. 

Since the proxies do not fully determine household income, obviously, there is bound to be inclusion and exclusion error. This should be even worse in a population with clusters, and we thus decided to test how high these errors would be by simulating a dataset. It contains data on income, divided in 18 clusters, each with its own random distribution, and other factors like education, average age, and family size that all contribute to determine income, plus an error term. We then simulate a PMT regressing income on our variables, and then go back and classify each household according to a threshold of income. We do this both for the true income and the estimated income, and, when they don't meet, we have a case of either inclusion or exclusion error, meaning our regression either assigned a household that should not have been assigned to the program, or it did not assign a household that should have received the benefit. We can then calculate the erros, and all of this is done in the _program.do_ file.

To test for robustness, however, we also wrote a _simulation.do_ file, which repeats the program many times and records the results. The idea here is to understand the range of variability of both errors, and to see how they behave as sample size increases. We then generate density plots and record the data that was used to generate them in csv files, as well as the average inclusion and exclusion errors for each sample size. This means that the final output of the program will be a set of k graphs and k cdv files, where k is the number of different sample sizes we tried (currently, we try 4 different sample sizes at k = 10, 50, 100, and 500). We can then use these data to run standard statistical measures on the variability, such as estimating the standard error, and, in the future, run power calculations. 